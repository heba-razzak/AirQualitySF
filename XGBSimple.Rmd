---
title: "Model Building"
output: github_document
---

# Model Building

```{r, setup, include=FALSE}
preprocessing_directory <- readr::read_file("inputs/preprocessing_directory.txt")
model_directory <- readr::read_file("inputs/model_directory.txt")
options(scipen = 999)
```

## Load required libraries
```{r, load-libraries, message = FALSE, warning = FALSE}
library(lubridate)      # For dates
library(dplyr)          # For data manipulation
library(tidyr)          # pivot
library(ggplot2)        # plots
library(data.table)     # Faster than dataframes (for big files)
library(xgboost)        # XGBoost model
library(caret)          # Preprocessing

library(caTools)
library(cvms)
```



## Load Dataset and Preprocessing
```{r, load-dataset}
dataset <- fread(paste0(preprocessing_directory, "/final_dataset_fire.csv"))
dataset <- dataset %>% select(-weatherstation)
```

```{r}
# Sort the dataset by time_stamp
dataset_xgb <- dataset %>%
  arrange(time_stamp)

# Set a 70% split by time_stamp
split_index <- floor(0.7 * nrow(dataset_xgb))
split_time <- dataset_xgb %>%
  pull(time_stamp) %>%
  .[split_index]

# Split the dataset into training and testing sets
train_set <- subset(dataset_xgb, time_stamp <= split_time)
test_set <- subset(dataset_xgb, time_stamp > split_time)

# Remove columns that won't be used in the model (time_stamp, sensor_index)
X_train <- train_set %>% select(-time_stamp, -sensor_index)

X_test <- test_set %>% select(-time_stamp, -sensor_index)

# Define target variable 
y_train <- X_train$pm2.5_atm 
y_test <- X_test$pm2.5_atm  

# Remove target variable from features
X_train <- X_train %>% select(-pm2.5_atm) 
X_test <- X_test %>% select(-pm2.5_atm) 
```

```{r}
# Convert training and test data into DMatrix format for XGBoost
xgb_train <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)
xgb_test <- xgb.DMatrix(data = as.matrix(X_test), label = y_test)
```

```{r}
xgb_params <- list(
  booster = "gbtree",
  eta = 0.01,            # Learning rate
  max_depth = 8,         # Maximum tree depth
  gamma = 4,             # Minimum loss reduction required to make a further partition
  subsample = 0.75,      # Subsample ratio of the training data
  colsample_bytree = 1,  # Subsample ratio of columns when constructing trees
  objective = "reg:squarederror",  # Use "reg:squarederror" for regression tasks
  eval_metric = "rmse"   # Root Mean Square Error as evaluation metric
)
```

```{r}
# Train the XGBoost model
xgb_model <- xgb.train(
  params = xgb_params,  # Your previously defined parameters
  data = xgb_train,     # Training data in DMatrix format
  nrounds = 5000,       # Number of boosting rounds
  verbose = 1           # Print information about the training process
)

# Print the model to inspect
xgb_model
```

```{r}
# Extract feature importance matrix for regression
importance_matrix <- xgb.importance(
  feature_names = colnames(X_train),  # Replace 'colnames(xgb_train)' with 'X_train' as we are working with matrices
  model = xgb_model
)

# Display feature importance
importance_matrix
```

```{r}
# Plot feature importance
xgb.plot.importance(importance_matrix)
```

```{r}
# Generate predictions for the test set
xgb_preds <- predict(xgb_model, as.matrix(X_test))

# Convert the predictions into a dataframe
xgb_preds <- as.data.frame(xgb_preds)
colnames(xgb_preds) <- "PredictedPM2.5"  # Assuming 'pm2.5_atm' is your target variable
xgb_preds$ActualPM2.5 <- y_test           # Actual values from the test set

# Display the first few rows of predictions
head(xgb_preds)
```

```{r}
# Calculate RMSE (Root Mean Square Error)
rmse <- sqrt(mean((xgb_preds$PredictedPM2.5 - xgb_preds$ActualPM2.5)^2))
print(paste("RMSE:", rmse))

# Calculate MAE (Mean Absolute Error)
mae <- mean(abs(xgb_preds$PredictedPM2.5 - xgb_preds$ActualPM2.5))
print(paste("MAE:", mae))

# Calculate the R-squared value
ss_res <- sum((xgb_preds$ActualPM2.5 - xgb_preds$PredictedPM2.5)^2)  # Sum of squares of residuals
ss_tot <- sum((xgb_preds$ActualPM2.5 - mean(xgb_preds$ActualPM2.5))^2)  # Total sum of squares

r_squared <- 1 - (ss_res / ss_tot)
print(paste("R-squared:", r_squared))
```


```{r}
# Plot predicted vs actual values
library(ggplot2)
ggplot(xgb_preds, aes(x = ActualPM2.5, y = PredictedPM2.5)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "red") +
  labs(title = "Predicted vs Actual PM2.5 Levels", x = "Actual PM2.5", y = "Predicted PM2.5") +
  theme_minimal()
```





















```{r, load-dataset}
# Read the dataset and select relevant features
dataset <- fread(paste0(preprocessing_directory, "/final_dataset.csv"))

# Preprocess the dataset for modeling
dataset_xgb <- dataset %>%
  rename(target = pm2.5_atm)

# Save the transformed dataset
saveRDS(dataset_xgb, file = paste0(model_directory, "/dataset_xgb.rds"))
```

## Train-Test Split
```{r, split-data}
# Determine the split index and split time
split_index <- floor(0.7 * nrow(arrange(dataset_xgb, time_stamp)))
split_time <- dataset_xgb %>%
  arrange(time_stamp) %>%
  pull(time_stamp) %>%
  .[split_index]

# Split the data into training and testing sets
train <- subset(dataset_xgb, time_stamp <= split_time)
test <- subset(dataset_xgb, time_stamp > split_time)

# Remove timestamp and sensor index from the features
train <- train %>%
  select(-time_stamp, -sensor_index)
test <- test %>%
  select(-time_stamp, -sensor_index)

# Save the split datasets
saveRDS(train, file = paste0(model_directory, "/train.rds"))
saveRDS(test, file = paste0(model_directory, "/test.rds"))
```

## Prepare Data for XGBoost
```{r, prepare-data}
# Extract labels and remove the target variable from features
train_labels <- train$target
test_labels <- test$target
train_matrix <- as.matrix(train %>% select(-target))
test_matrix <- as.matrix(test %>% select(-target))

# Save matrices and labels
saveRDS(train_matrix, file = paste0(model_directory, "/train_matrix.rds"))
saveRDS(test_matrix, file = paste0(model_directory, "/test_matrix.rds"))
saveRDS(train_labels, file = paste0(model_directory, "/train_labels.rds"))
saveRDS(test_labels, file = paste0(model_directory, "/test_labels.rds"))
```

## Model Training with XGBoost
```{r, train-model}
# Load the matrices and labels
train_matrix <- readRDS(paste0(model_directory, "/train_matrix.rds"))
test_matrix <- readRDS(paste0(model_directory, "/test_matrix.rds"))
train_labels <- readRDS(paste0(model_directory, "/train_labels.rds"))
test_labels <- readRDS(paste0(model_directory, "/test_labels.rds"))

# Create DMatrix objects
dtrain <- xgb.DMatrix(data = train_matrix, label = train_labels)
dtest <- xgb.DMatrix(data = test_matrix, label = test_labels)

# Set model parameters
params <- list(
  booster = "gbtree",
  objective = "reg:squarederror",
  eta = 0.1,
  max_depth = 6,
  subsample = 1,
  colsample_bytree = 1
)

# Perform cross-validation to find the best number of rounds
xgbcv <- xgb.cv(
  params = params,
  data = dtrain,
  nrounds = 100,
  nfold = 5,
  showsd = TRUE,
  early_stopping_rounds = 20,
  maximize = FALSE,
  print_every_n = 10
)

# Train the final model
final_model <- xgboost(
  params = params,
  data = dtrain,
  nrounds = xgbcv$best_iteration,
  watchlist = list(train = dtrain, test = dtest),
  early_stopping_rounds = 20,
  maximize = FALSE
)

# Save the trained model
saveRDS(final_model, file = paste0(model_directory, "/final_model.rds"))
```

## Feature Importance
```{r, feature-imp}
# Calculate and plot feature importance
importance <- xgb.importance(feature_names = colnames(train_matrix), model = final_model)
xgb.plot.importance(importance_matrix = importance)

# Save feature importance
saveRDS(importance, file = paste0(model_directory, "/feature_importance.rds"))
```




```{r}
dataset <- fread(paste0(preprocessing_directory, "/final_dataset.csv"))
dataset <- dataset %>% select(-date_created, -last_seen, -rssi, -uptime, -memory)
```

```{r}
# Read and preprocess the dataset
dataset_xgb <- dataset %>% rename(target = pm2.5_atm)

# Save the transformed dataset
saveRDS(dataset_xgb, file = paste0(model_directory, "/dataset_xgb.rds"))
```

```{r}
# Load transformed dataset
# dataset_xgb <- readRDS(paste0(model_directory, "/dataset_xgb.rds"))

# Determine the index for the 70% split
split_index <- floor(0.7 * nrow(arrange(dataset_xgb, time_stamp)))

# Find the date at this index
split_time <- dataset_xgb %>% arrange(time_stamp) %>%
  pull(time_stamp) %>% .[split_index]

# Split the data into training and testing sets based on the split_time
train <- subset(dataset_xgb, time_stamp <= split_time)
test <- subset(dataset_xgb, time_stamp > split_time)

# Remove timestamp and sensor index from the features
train <- train %>% select(-time_stamp, -sensor_index)
test <- test %>% select(-time_stamp, -sensor_index)

# Save split datasets
saveRDS(train, file = paste0(model_directory, "/train.rds"))
saveRDS(test, file = paste0(model_directory, "/test.rds"))
```

```{r}
# # Load split datasets
# train <- readRDS(paste0(model_directory, "/train.rds"))
# test <- readRDS(paste0(model_directory, "/test.rds"))

# Extract labels and remove target from features
train_labels <- train$target
test_labels <- test$target

train_matrix <- as.matrix(train %>% select(-target))
test_matrix <- as.matrix(test %>% select(-target))

# Save matrices and labels
saveRDS(train_matrix, file = paste0(model_directory, "/train_matrix.rds"))
saveRDS(test_matrix, file = paste0(model_directory, "/test_matrix.rds"))
saveRDS(train_labels, file = paste0(model_directory, "/train_labels.rds"))
saveRDS(test_labels, file = paste0(model_directory, "/test_labels.rds"))
```

```{r}
# Load matrices and labels
train_matrix <- readRDS(paste0(model_directory, "/train_matrix.rds"))
test_matrix <- readRDS(paste0(model_directory, "/test_matrix.rds"))
train_labels <- readRDS(paste0(model_directory, "/train_labels.rds"))
test_labels <- readRDS(paste0(model_directory, "/test_labels.rds"))

# # Scale data based on training dataset
# scaler <- preProcess(train_matrix, method = c("center", "scale"))
# train_matrix <- predict(scaler, train_matrix)
# test_matrix <- predict(scaler, test_matrix)

# Create DMatrix objects
dtrain <- xgb.DMatrix(data = train_matrix, label = train_labels)
dtest <- xgb.DMatrix(data = test_matrix, label = test_labels)

print("Initial Cross-Validation")

# Initial cross-validation to find the best number of rounds
params <- list(
  booster = "gbtree",
  objective = "reg:squarederror",
  eta = 0.1,
  gamma = 0,
  max_depth = 6,
  min_child_weight = 1,
  subsample = 1,
  colsample_bytree = 1
)

xgbcv <- xgb.cv(
  params = params,
  data = dtrain,
  nrounds = 100,
  nfold = 5,
  showsd = TRUE,
  stratified = FALSE,
  print_every_n = 10,
  early_stopping_rounds = 20,
  maximize = FALSE
)

saveRDS(xgbcv, file = paste0(model_directory, "/xgbcv.rds"))
```

```{r}
# Load xgbcv
xgbcv <- readRDS(paste0(model_directory, "/xgbcv.rds"))

print("Cross-validation completed")
print(xgbcv$evaluation_log)

best_nrounds <- xgbcv$best_iteration
print(paste("Best number of rounds:", best_nrounds))

print("Hyperparameter Tuning with MLR")

# Create tasks
train_df <- as.data.frame(cbind(train_matrix, target = train_labels))
test_df <- as.data.frame(cbind(test_matrix, target = test_labels))
traintask <- makeRegrTask(data = train_df, target = "target")
testtask <- makeRegrTask(data = test_df, target = "target")

# Save tasks
saveRDS(traintask, file = paste0(model_directory, "/traintask.rds"))
saveRDS(testtask, file = paste0(model_directory, "/testtask.rds"))

# Load tasks
# traintask <- readRDS(paste0(model_directory, "/traintask.rds"))
# testtask <- readRDS(paste0(model_directory, "/testtask.rds"))

# Create learner
lrn <- makeLearner("regr.xgboost", predict.type = "response")
lrn$par.vals <- list(objective = "reg:squarederror", eval_metric = "rmse", nrounds = best_nrounds)

# Set parameter space
params <- makeParamSet(
  makeDiscreteParam("booster", values = c("gbtree", "gblinear")),
  makeIntegerParam("max_depth", lower = 3L, upper = 10L),
  makeNumericParam("min_child_weight", lower = 1L, upper = 10L),
  makeNumericParam("subsample", lower = 0.5, upper = 1),
  makeNumericParam("colsample_bytree", lower = 0.5, upper = 1)
)

# Set resampling strategy
rdesc <- makeResampleDesc("CV", stratify = FALSE, iters = 5L)

# Search strategy
ctrl <- makeTuneControlRandom(maxit = 10L)

# Parameter tuning
print("Starting parameter tuning")
mytune <- tuneParams(
  learner = lrn,
  task = traintask,
  resampling = rdesc,
  par.set = params,
  control = ctrl,
  show.info = TRUE
)

saveRDS(mytune, file = paste0(model_directory, "/mytune.rds"))

print("Parameter tuning completed")
print(mytune)

# Set hyperparameters
lrn_tune <- setHyperPars(lrn, par.vals = mytune$x)

# Train model
print("Training final model")
xgmodel <- mlr::train(learner = lrn_tune, task = traintask)

saveRDS(xgmodel, file = paste0(model_directory, "/xgmodel.rds"))

# Predict model
print("Making predictions on test set")
xgpred <- predict(xgmodel, testtask)

saveRDS(xgpred, file = paste0(model_directory, "/xgpred.rds"))

# Load trained model and predictions
# xgmodel <- readRDS(paste0(model_directory, "/xgmodel.rds"))
# xgpred <- readRDS(paste0(model_directory, "/xgpred.rds"))

# Calculate performance metrics
rmse_original <- sqrt(mean((xgpred$data$response - test_labels)^2))
print(paste("RMSE on original scale:", rmse_original))

rss <- sum((xgpred$data$response - test_labels)^2)
tss <- sum((test_labels - mean(test_labels))^2)
r_squared <- 1 - (rss / tss)
print(paste("R-squared on original scale:", r_squared))

n <- length(test_labels)
p <- ncol(test_matrix)
adjusted_r_squared <- 1 - ((1 - r_squared) * (n - 1) / (n - p - 1))
print(paste("Adjusted R-squared on original scale:", adjusted_r_squared))

# Plot residuals
print("Plotting residuals")
residuals <- test_labels - xgpred$data$response
plot(residuals, main = "Residuals Plot", ylab = "Residuals", xlab = "Index", col = "blue", pch = 20)
abline(h = 0, col = "red")

# Histogram of residuals
hist(residuals, breaks = 50, main = "Histogram of Residuals", xlab = "Residuals", col = "blue")

# Plot predictions vs actual values
plot(test_labels, xgpred$data$response, main = "Predictions vs Actual Values", xlab = "Actual Values", ylab = "Predicted Values", col = "blue", pch = 19)
abline(0, 1, col = "red")

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```
