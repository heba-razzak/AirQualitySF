---
title: "model"
output: html_document
date: "2023-12-12"
---

# Land Use Regression Model

# Load required libraries
```{r load packages}
# {r, echo=FALSE} hides code, only shows output
suppressPackageStartupMessages({
  library(lubridate) # For dates
  library(dplyr) # For data manipulation
  library(sf) # For working with spatial data
  library(timeDate) # For holidays
  library(mapview) # to view maps
  library(tidyr) # pivot
  library(ggplot2) # plots
  library(data.table) # Faster than dataframes (for big files) 
})

holidays = holidayNYSE(2019)
```

# Read PurpleAir files
```{r read purpleair}
# Set the directory path
directory_path <- "/Users/heba/Desktop/Uni/Lim Lab/Purple Air"

# Get a list of files that start with "purple_air_sanfran_"
file_list <- list.files(directory_path, pattern = "purple_air_sanfran_")

# Initialize an empty list to store dataframes
data_list <- list()

# Loop through each file and read it into a dataframe
for (file in file_list) {
  file_path <- file.path(directory_path, file)
  data <- fread(file_path)
  data_list[[file]] <- data
}

# Combine all dataframes into one
purpleair <- bind_rows(data_list)

# Print the first few rows of the combined dataframe
head(purpleair)

```

# Read Uber Files (2018)
```{r read uber}
# Set the directory path
directory_path <- "/Users/heba/Desktop/Uni/Lim Lab/Uber/Speeds"

# Get a list of files that start with "movement..."
# file_list <- list.files(directory_path, pattern = "movement-speeds-hourly-san-francisco-2018")
file_list <- c("movement-speeds-hourly-san-francisco-2018-1.csv","movement-speeds-hourly-san-francisco-2018-2.csv",
               "movement-speeds-hourly-san-francisco-2018-3.csv")
# Initialize an empty list to store dataframes
data_list_2018 <- list()
file = file_list[1]
# Loop through each file and read it into a dataframe
for (file in file_list) {
  file_path <- file.path(directory_path, file)
  data <- fread(file_path)
  data <- data %>% select(utc_timestamp,osm_way_id,speed_mph_mean)
  data <- data[complete.cases(data), ]
  # data$utc_timestamp <- ymd_hms(data$utc_timestamp)
  data$osm_way_id <- as.character(data$osm_way_id)
  data_list_2018[[file]] <- data
}

# Combine all dataframes into one
uber_data <- rbindlist(data_list_2018, use.names = TRUE, fill = TRUE)

# Print the first few rows of the combined dataframe
head(uber_data)
```
# Free flow speeds
```{r}
uber_data <- uber_data %>%
  group_by(osm_way_id) %>%
  mutate(free_flow_speed = quantile(speed_mph_mean, 0.95)) %>%
  ungroup()
```

# Calculate congestion ratio for each observation (1 = free flow speed, <1 = congestion, >1 = faster speed)
```{r Calculate congestion}
uber_data$congestion_ratio <- uber_data$speed_mph_mean / uber_data$free_flow_speed
```

```{r}
head(uber_data)
```

# Aggregate congestion ratio by timestamp
```{ Aggregate congestion ratio by timestamp}
road_congestion_hourly <- uber_data %>%
  group_by(osm_way_id, utc_timestamp) %>%
  summarise(
    congestion_ratio_mean = mean(congestion_ratio, na.rm = TRUE)
  ) %>%
  ungroup()
```
# Visualize congestion by hour and day
```{r}
uber_data$local_timestamp <- with_tz(uber_data$utc_timestamp, tzone = "America/Los_Angeles")

road_congestion_dailyhourly <- uber_data %>%
  mutate(DayOfWeek = factor(wday(local_timestamp)),
         HourOfDay = hour(local_timestamp)) %>% 
  group_by(DayOfWeek, HourOfDay) %>%
  summarize(congestion_ratio_mean = mean(congestion_ratio)) %>%
  ungroup()
```

```{r}
(road_congestion_dailyhourly)
```

```{r}
heatmap_plot <- ggplot(road_congestion_dailyhourly, aes(x = HourOfDay, y = DayOfWeek, fill = congestion_ratio_mean)) +
  geom_tile() +
  scale_fill_gradientn(
  colours=c("red", "yellow", "green")) +
  labs(
    title = "Congestion Heatmap (local time)",
    x = "Hour of Day",
    y = "Day of Week"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(heatmap_plot)
```


```{r Visualize congestion by hour and day}

road_congestion_hourly$local_timestamp <- with_tz(road_congestion_hourly$utc_timestamp, tzone = "America/Los_Angeles")

road_congestion_hm <- road_congestion_hourly %>%
  mutate(DayOfWeek = factor(wday(local_timestamp, label = TRUE, abbr = FALSE)),
         HourOfDay = hour(local_timestamp)) %>% 
  group_by(DayOfWeek, HourOfDay) %>%
  summarize(congestion_ratio_mean = mean(congestion_ratio_mean)) %>%
  ungroup()

road_congestion_hourly <- road_congestion_hourly %>% select(-local_timestamp)

heatmap_plot <- ggplot(road_congestion_hm, aes(x = HourOfDay, y = DayOfWeek, fill = congestion_ratio_mean)) +
  geom_tile() +
  scale_fill_gradientn(
  colours=c("red", "yellow", "green")) +
  labs(
    title = "Congestion Heatmap (local time)",
    x = "Hour of Day",
    y = "Day of Week"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(heatmap_plot)

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```







##########################################

# Temperature data
```{r read and filter temps}
temps <- read.csv("/Users/heba/Desktop/Uni/Lim Lab/SanFranTemp.csv")
temps <- temps %>% mutate(Date = ymd(Date)) %>% filter(year(Date) %in% c(2018,2019)) %>%
  mutate(datetime = ymd_hms(paste(Date, Time))) %>%
  select(datetime, everything(), -Date, -Time)
```

```{r}
uber_data <- uber_data %>%
  group_by(osm_way_id) %>%
  mutate(free_flow_speed = quantile(speed_mph_mean, 0.95)) %>%
  ungroup()
```

```{r}

```

# read spatial data
```{r read spatial data}
path = '/Users/heba/Desktop/Uni/Lim Lab/uber_purpleair/'
# selected_ways_sf <- st_read(paste0(path,'selected_ways_sf.shp'))
purpleairs_sf <- st_read(paste0(path,'purpleairs_sf.shp'))
# uber_ways_sf <- st_read(paste0(path,'uber_ways_sf.shp'))
# purpleairs_buffers <- st_read(paste0(path,'purpleairs_buffers.shp'))
# intersections <- st_read(paste0(path,'intersections.shp'))

purpleair_uber_roads <- st_read(paste0(path,'purpleair_uber_roads.shp'))
purpleair_road_length <- st_read(paste0(path,'purpleair_road_length.shp'))
purpleair_buildings <- st_read(paste0(path,'purpleair_buildings.shp'))
purpleair_building_areas <- st_read(paste0(path,'purpleair_building_areas.shp'))


sanfran_roads <- st_read("/Users/heba/Desktop/Uni/Lim Lab/OSM/sanfranarea_roads_osm.shp")
```
# Converting timestamps to a date-time format
```{r timestamps format}
purpleair$time_stamp <- ymd_hms(purpleair$time_stamp)
```

# Convert the OSM way IDs in the Uber speeds data to character type
```{r OSM way IDs to char}
# uber_data$osm_way_id <- as.character(uber_data$osm_way_id)
```
# Calculate free-flow speed (95th percentile of speed) for each osm_way_id
```{r Calculate free-flow speed}
free_flow_speeds <- uber_data %>% select(osm_way_id, speed_mph_mean) %>%
  group_by(osm_way_id) %>%
  summarise(free_flow_speed = quantile(speed_mph_mean, 0.95)) %>%
  ungroup()
```

# Join free flow speeds to Uber ways spatial data
```{r Join free flow speeds to Uber ways}
purpleair_uber_roads$osm_id <- as.character(purpleair_uber_roads$osm_id)

speed_map <- purpleair_uber_roads %>% left_join(free_flow_speeds, by = c("osm_id" = "osm_way_id"))
```
# Visualize the speed map
```{r speed map}
mapview(speed_map, zcol="free_flow_speed")
```
# Join free_flow_speeds with uber_data
```{r join free_flow_speeds w uber}
uber_data <- uber_data %>%
  left_join(free_flow_speeds, by = "osm_way_id")
```
# Calculate congestion ratio for each observation (1 = free flow speed, <1 = congestion, >1 = faster speed)
```{r Calculate congestion}
uber_data$congestion_ratio <- uber_data$speed_mph_mean / uber_data$free_flow_speed
```
# Aggregate congestion ratio by timestamp
```{r Aggregate congestion ratio by timestamp}
road_congestion_hourly <- uber_data %>%
  group_by(osm_way_id, utc_timestamp) %>%
  summarise(
    congestion_ratio_mean = mean(congestion_ratio, na.rm = TRUE)
  ) %>%
  ungroup()
```
# Visualize congestion by hour and day

```{r Visualize congestion by hour and day}

road_congestion_hourly$local_timestamp <- with_tz(road_congestion_hourly$utc_timestamp, tzone = "America/Los_Angeles")

road_congestion_hm <- road_congestion_hourly %>%
  mutate(DayOfWeek = factor(wday(local_timestamp, label = TRUE, abbr = FALSE)),
         HourOfDay = hour(local_timestamp)) %>% 
  group_by(DayOfWeek, HourOfDay) %>%
  summarize(congestion_ratio_mean = mean(congestion_ratio_mean)) %>%
  ungroup()

road_congestion_hourly <- road_congestion_hourly %>% select(-local_timestamp)

heatmap_plot <- ggplot(road_congestion_hm, aes(x = HourOfDay, y = DayOfWeek, fill = congestion_ratio_mean)) +
  geom_tile() +
  scale_fill_gradientn(
  colours=c("red", "yellow", "green")) +
  labs(
    title = "Congestion Heatmap (local time)",
    x = "Hour of Day",
    y = "Day of Week"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(heatmap_plot)

```


# Maps (Select Relevant Layers)
```{r buildings map}
purpleair_buildings <- purpleair_buildings %>%
  mutate(building_type = type) %>%
  select(-type) %>% 
  mutate(building_type = coalesce(building_type, 'NA'))

purpleair_uber_roads <- purpleair_uber_roads %>%
  mutate(road_type = type) %>%
  select(-type) %>%
  mutate(road_type = coalesce(road_type, 'NA'))

pa_sensors <- purpleairs_sf %>% select(sensr_d)

palette1 <- colorRampPalette(c("pink"))
palette2 <- colorRampPalette(c("lightgrey"))
palette3 <- colorRampPalette(c("red", "green"))

# Create maps with custom color palettes
map1 <- mapview(purpleair_buildings)
map2 <- mapview(purpleair_uber_roads, zcol = "road_type", col.regions = palette2,col = palette2)
map3 <- mapview(speed_map, zcol = "free_flow_speed", col.regions = palette3, col = palette3)
map4 <- mapview(pa_sensors, col.regions = "purple", legend = FALSE)

# Combine the maps
combined_map <- map1 + map2 + map3 + map4

combined_map
```

# Join PurpleAir Uber Temp OSM
```{r Join PurpleAir Uber Temp OSM}
# purpleair_uber_roads - "osm_id","name","type","sensr_d","dt_crtd","last_sn","geometry"
# road_congestion_hourly - "osm_way_id","utc_timestamp","congestion_ratio_mean"
# purpleair_road_length - "sensr_d","type","road_length","geometry"   
# purpleair_building_areas - "sensr_d","type","total_area","geometry"  
# purpleair - "time_stamp","pm1.0_atm","pm2.5_atm","pm2.5_atm_a","pm2.5_atm_b","sensor_id"
# temps - "datetime","TemperatureFahrenheit","TemperatureCelsius"   

purpleair2 <- purpleair %>% select(time_stamp,sensor_id,pm2.5_atm)
temps2 <- temps %>% select(datetime,TemperatureFahrenheit)
purpleair_building_areas2 <- st_drop_geometry(purpleair_building_areas) %>% select(sensr_d,type,total_area)
purpleair_road_length2 <- st_drop_geometry(purpleair_road_length) %>% select(sensr_d,type,road_length = rd_lngt)
# road_congestion_hourly # osm_way_id, utc_timestamp, congestion_ratio_mean
purpleair_uber <- st_drop_geometry(purpleair_uber_roads) %>% select(sensr_d,osm_id)

# pivot area by building type
# purpleair_building_areas %>%
#   group_by(sensr_d, type) %>%
#   summarize(total_area = sum(total_area)) %>%
#   pivot_wider(names_from = type, values_from = total_area, names_prefix = "area_")

purpleair_building_areas3 <- purpleair_building_areas2 %>%
  group_by(sensr_d) %>%
  summarize(total_area = sum(total_area)) %>% ungroup()

# pivot length by road type
purpleair_road_length3 <- purpleair_road_length2 %>%
  group_by(sensr_d, type) %>%
  summarize(road_length = sum(road_length)) %>%
  pivot_wider(names_from = type, values_from = road_length, names_prefix = "length_") %>% ungroup()

purpleair_road_length4 <- purpleair_road_length2 %>%
  group_by(sensr_d) %>%
  summarize(road_length = sum(road_length)) %>% ungroup()

pa_temp <- left_join(purpleair, temps, by = c("time_stamp" = "datetime"))
pa_congestion <- left_join(purpleair_uber, road_congestion_hourly, by = c("osm_id" = "osm_way_id"))

# check how i calculate this
# alternative to getting the mean of mean
pa_congestion <- pa_congestion %>% group_by(sensr_d, utc_timestamp) %>% summarize(mean_congestion = mean(congestion_ratio_mean)) %>% ungroup()

result <- left_join(pa_temp, pa_congestion, by = c("time_stamp" = "utc_timestamp", "sensor_id" = "sensr_d"))
result <- left_join(result, purpleair_building_areas3, by = c("sensor_id" = "sensr_d"))
result <- left_join(result, purpleair_road_length3, by = c("sensor_id" = "sensr_d"))

# head(result, 10)

```

# Analyze missing values
```{r Analyze missing values}

all_na_columns <- names(result)[colSums(is.na(result)) == nrow(result)]
cat("columns with all NA:\n",all_na_columns)
result <- result %>% dplyr::select(-all_of(all_na_columns))

missing_count <- colSums(is.na(result))
non_missing_count <- colSums(!is.na(result))

data_quality_summary <- data.frame(
  Column = colnames(result),
  Status = factor(rep(c("Missing", "Non-Missing"), each = ncol(result))),
  Count = c(missing_count, non_missing_count)
)

ggplot(data_quality_summary, aes(x = Column, fill = Status, y = Count)) +
  geom_bar(stat = "identity", position = "stack") +
  xlab("Column") +
  ylab("Count") +
  scale_fill_manual(values = c("Missing" = "red", "Non-Missing" = "green")) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```
# Handle Missing Values
```{r Handle Missing Values}
# Since most missing values are lengths of roads
# Fill missing "length" values with 0

# column names starting with length
cols_to_fill_na <- grep("^length_", names(result), value = TRUE)

# Replace NA with 0 in selected columns
result <- result %>%
  mutate_at(vars(all_of(cols_to_fill_na)), ~replace_na(., 0))

# replace missing mean_congestion with 1 (no traffic)
result <- result %>%
  mutate_at(vars(mean_congestion), ~replace_na(., 1))
```

# Number of purple air sensors
``` {r}
unique_sensor_count <- result %>%
  select(sensor_id) %>%
  distinct() %>%
  n_distinct()

cat("Number of PurpleAir sensors: ",unique_sensor_count)
```

# Create date variables for prediction
```{r}
result$local_timestamp <- with_tz(result$time_stamp, tzone = "America/Los_Angeles")

# # no holidays in june so it doesnt make sense to use for this month
# result$local_date <- as.Date(result$local_timestamp)
# result$is_holiday <- result$local_date %in% as.Date(holidays)

result$day_of_week <- wday(result$local_timestamp)
result$hour <- hour(result$local_timestamp)
result$day <- day(result$local_timestamp)
result$month <- month(result$local_timestamp)
result$year <- year(result$local_timestamp)
result$is_weekend <- ifelse(result$day_of_week %in% c(6, 7), 1, 0) # 6 and 7 represent the weekend

head(result)
```

# Check readings where channel A and B disagree 
## plot of channel A vs B
```{r}

result$pm2.5dif <- abs(result$pm2.5_atm_a-result$pm2.5_atm_b)

look_into_this <- result %>% filter(pm2.5dif > 15) %>% select(sensor_id,pm2.5dif, pm2.5_atm_a,pm2.5_atm_b,TemperatureCelsius,mean_congestion,local_timestamp )

result <- result %>% select(-pm2.5dif) # remove dif from result

head(look_into_this)

ggplot(result, aes(x = pm2.5_atm_a, y = pm2.5_atm_b)) +
  geom_point() +
  labs(x = "Channel A PM2.5",
       y = "Channel B PM2.5") +
  theme_minimal()
```


```{r}
# Create a subset of the data for each sensor_id
sensor_ids <- unique(result$sensor_id)

# Create a list to store the plots
plots <- list()

# Loop through each sensor_id and create a plot
for (sensor_id in sensor_ids) {
  subset_data <- subset(result, sensor_id == sensor_id)
  
  # Create a plot for pm2.5_atm_a
  plot_a <- ggplot(subset_data, aes(x = local_timestamp, y = pm2.5dif)) +
    geom_line() +
    labs(title = paste("Sensor ID:", sensor_id, "- pm2.5_diff"),
         x = "Local Timestamp",
         y = "pm2.5_atm_a")
  
  # Add the combined plot to the list of plots
  plots[[sensor_id]] <- plot_a
}

# Display the plots
# plots


```


# Final dataset for model
```{r}
result <- result %>% select(-pm1.0_atm,-pm2.5_atm_a,-pm2.5_atm_b,-sensor_id,-TemperatureCelsius,-time_stamp)
glimpse(result)
```

# Split Train and Test data
```{r}
suppressPackageStartupMessages({
  library(caTools)
})
set.seed(42)

# Define the time point at which to split the data (e.g., 70% for training)
split_time <- quantile(result$local_timestamp, 0.7)

# Split the data into training and testing sets based on the split_time
train_data <- subset(result, local_timestamp <= split_time)
test_data <- subset(result, local_timestamp > split_time)

train_data <- train_data %>% select(-local_timestamp)
test_data <- test_data %>% select(-local_timestamp)

# split <- sample.split(result$pm2.5_atm, SplitRatio = 0.7)
# train_data <- subset(result, split == TRUE)
# test_data  <- subset(result, split == FALSE)
```

# Random Forest Model

```{r}
# Train random forest model
suppressPackageStartupMessages({
  library(randomForest)
})
set.seed(42)
rf_model <- randomForest(pm2.5_atm ~ ., data = train_data, ntree = 500)

# Make random forest predictions
predictions_rf <- predict(rf_model, test_data)

# Mean Absolute Error

MAE <- mean(abs(predictions_rf - test_data$pm2.5_atm))

# Compute R squared

R2 <- 1 - sum((test_data$pm2.5_atm - predictions_rf)^2) / sum((test_data$pm2.5_atm - mean(test_data$pm2.5_atm))^2)

# Print R squared

cat("R squared:", R2)

# Visualize predictions

suppressPackageStartupMessages({
  library(ggplot2)
})

# plot(predictions_rf,test_data$pm2.5_atm)

ggplot(data = test_data, aes(x = pm2.5_atm, y = predictions_rf)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "red") +  # Add a diagonal line for reference
  labs(x = "Actual PM2.5", y = "Predicted PM2.5") +
  ggtitle("Scatter Plot of Predicted vs. Actual PM2.5") +
  theme_minimal()
```

# Train XGBoost model
```{r}
suppressPackageStartupMessages({
  library(xgboost)
})
set.seed(42)
target_variable <- "pm2.5_atm"
predictor_variables <- setdiff(names(train_data), target_variable)

train_matrix <- xgb.DMatrix(data = as.matrix(train_data[, predictor_variables]), label = train_data[, target_variable])
test_matrix <- xgb.DMatrix(data = as.matrix(test_data[, predictor_variables]))

# Define XGBoost parameters
xgb_params <- list(
  objective = "reg:squarederror",  # For regression tasks
  eval_metric = "rmse",            # Root Mean Squared Error as the evaluation metric
  eta = 0.1,                       # Learning rate (adjust as needed)
  max_depth = 6,                   # Maximum depth of trees (adjust as needed)
  nrounds = 500,                   # Number of boosting rounds (adjust as needed)
  verbosity = 0                    # Set to 0 to suppress output
)

# Train the XGBoost model
xgb_model <- xgboost(data = train_matrix, params = xgb_params, nrounds = xgb_params$nrounds)

predictions_xgb <- predict(xgb_model, newdata = test_matrix)

# Calculate MAE
MAE <- mean(abs(predictions_xgb - test_data$pm2.5_atm))

# Calculate RMSE
RMSE <- sqrt(mean((predictions_xgb - test_data$pm2.5_atm)^2))

# Calculate R-squared (R²)
SSE <- sum((predictions_xgb - test_data$pm2.5_atm)^2)
SST <- sum((test_data$pm2.5_atm - mean(test_data$pm2.5_atm))^2)
R2 <- 1 - SSE / SST

cat("Mean Absolute Error (MAE):", MAE, "\n")
cat("Root Mean Squared Error (RMSE):", RMSE, "\n")
cat("R-squared (R²):", R2, "\n")

ggplot(data = test_data, aes(x = pm2.5_atm, y = predictions_xgb)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "red") +  # Add a diagonal line for reference
  labs(x = "Actual PM2.5", y = "Predicted PM2.5") +
  ggtitle("Scatter Plot of Predicted vs. Actual PM2.5") +
  theme_minimal()
```

# XGBoost Feature importance
``` {r}
importance_scores <- xgb.importance(model = xgb_model)
xgb.plot.importance(importance_matrix = importance_scores)
```


# Investigate PM2.5 readings
``` {r}

hist_data <- hist(result$pm2.5_atm, breaks = 50, xlab = "PM2.5 Concentration", xaxt = 'n')
axis(side = 1, at = hist_data$mids, labels = hist_data$mids)

hist_table <- data.frame(
  Bin_Start = hist_data$breaks[-length(hist_data$breaks)],
  Bin_End = hist_data$breaks[-1],
  Frequency = hist_data$counts
)

# Print the histogram data as a table
print(hist_table)
```
